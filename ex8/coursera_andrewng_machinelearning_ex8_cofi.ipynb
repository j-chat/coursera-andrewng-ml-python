{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Programming Exercise 8: Recommender Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import package(s)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio \n",
    "import scipy.stats\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Recommender Systems\n",
    "In this part of the exercise, the collaborative filtering learning algorithm is applied to a dataset of movie ratings from [GroupLens Research](https://grouplens.org/datasets/movielens/). This dataset consists of ratings on a scale of 1 to 5. The dataset has $n_u$ = 943 users and $n_m$ = 1682 movies.\n",
    "\n",
    "The function cofiCostFunc will compute the collaborative filtering objective function and gradient. Afterwards, an optimizing algorithm will learn the parameters for collaborative filtering.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    \n",
    "   One important application of machine learning is building better recommender systems. Improving the performance of a recommender system can have a substantial and immediate impact (increased revenue, better service, etc.) of many companies.\n",
    "   \n",
    "   There are two common learning algorithms used in recommender systems: content based and collaborative filtering:\n",
    "   * Content based learning algorithm uses the content of the products/servcies as features to make predictions for a user's ratings. For movie recommendations, the content of the movies could be how much action/romance/etc. is in the movie. Yet, it may be difficult to get these features if they even exist. \n",
    "   * Collaborative filtering learning algorithm  can start to learn for itself what features to use. It uses a large set of users' ratings collaboratively to learn better ratings (features) for everyone. For movie recommendations, many users rating some subset of movies is helping the algorithm learn better features to make better movie predictions for everyone else. \n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Movie ratings dataset\n",
    "The dataset ex8 movies.mat contains the variables $Y$ and $R$. The matrix $Y$ (a num movies$\\times$num users matrix) stores the ratings $y^{(i,j)}$ (from 1 to 5). The matrix $R$ is an binary-valued indicator matrix, where $R(i,j)=1$ if user $j$ gave a rating to movie $i$, and $R(i,j)=0$ otherwise. The objective of collaborative filtering is to predict movie ratings for the movies that users have not yet rated, that is, the entries with $R(i,j)=0$. This will allow us to recommend the movies with the highest predicted ratings to the user. \n",
    "\n",
    "To understand the matrix $Y$, compute the average movie rating for the first movie (Toy Story) and output the average rating to the screen.\n",
    "\n",
    "This exercise uses the matrix $X$ and $Theta$:\n",
    "\n",
    "$X= \\begin{bmatrix}\n",
    "       -(x^{(1)})^T- \\\\\n",
    "       -(x^{(2)})^T- \\\\\n",
    "       \\vdots \\\\\n",
    "       -(x^{(n_m)})^T-\n",
    "       \\end{bmatrix}, \\quad Theta = \\begin{bmatrix}\n",
    "       -(\\theta^{(1)})^T- \\\\\n",
    "       -(\\theta^{(2)})^T- \\\\\n",
    "       \\vdots \\\\\n",
    "       -(\\theta^{(n_u)})^T-\n",
    "       \\end{bmatrix}$\n",
    "       \n",
    "The $i$-th row of $X$ corresponds to the feature vector $x^{(i)}$ for the $i$-th movie, and the $j$-th row of $Theta$ corresponds to one parameter vector $\\theta^{(j)}$ for the $j$-th user. Both $x^{(i)}$ and $\\theta^{(j)}$ are $n$-dimensional vectors. For the purposes of this exercise, $n=100$. Therefore, $x^{(i)} \\in \\mathbb{R}^{100}$ and $\\theta^{(j)} \\in \\mathbb{R}^{100}$. Correspondingly, $X$ is a $n_m \\times 100$ matrix and $Theta$ is a $n_u \\times 100$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading movie ratings dataset.\n",
      "Average rating for movie 1 (Toy Story): 4.52 / 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Users')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============== Part 1: Loading movie ratings dataset ================\n",
    "#  You will start by loading the movie ratings dataset to understand the\n",
    "#  structure of the data.\n",
    "\n",
    "print('Loading movie ratings dataset.')\n",
    "\n",
    "#Load data\n",
    "movie_data = sio.loadmat('ex8_movies.mat')\n",
    "\n",
    "#Y is a 1682x943 matrix, containing ratings (1-5) of 1682 movies on \n",
    "#943 users\n",
    "#R is a 1682x943 matrix, where R(i,j) = 1 if and only if user j gave a\n",
    "#rating to movie i\n",
    "Y = movie_data['Y']\n",
    "R = movie_data['R']\n",
    "\n",
    "#From the matrix, we can compute statistics like average rating.\n",
    "print('Average rating for movie 1 (Toy Story): %.2f / 5'% np.mean(Y[0,R[0,:]]))\n",
    "\n",
    "#We can \"visualize\" the ratings matrix by plotting it with imagesc\n",
    "plt.figure()\n",
    "#Display the original image \n",
    "plt.imshow(Y)\n",
    "plt.ylabel('Movies')\n",
    "plt.xlabel('Users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is a visual of the movie ratings matrix $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Collaborative ﬁltering learning algorithm\n",
    "For the collaborative filtering learning algorithm, the cost function is first implemented without regularization. \n",
    "\n",
    "The collaborative filtering algorithm in the setting of movie recommendations considers a set of $n$-dimensional parameter vectors $x^{(1)}, \\dots, x^{(n_m)}$ and $\\theta^{(1)}, \\dots, \\theta^{(n_u)}$, where the model predicts the rating for movie $i$ by user $j$ as $y^{(i,j)} = (\\theta^{(j)})^T x^{(i)}$. Given a dataset that consists of a set of ratings produced by some users on some movies, our wish is to learn the parameter vectors $x^{(1)}, \\dots, x^{(n_m)}$,$\\theta^{(1)}, \\dots, \\theta^{(n_u)}$ that produce the best fit (minimizes the squared error). \n",
    "\n",
    "Use the function cofiCostFunc to compute the cost function and gradient for collaborative filtering. Note that the parameters to the function (i.e., the values that we are trying to learn) are $X$ and $Theta$. The cost function has been set up to unroll the parameters into a single vector $params$. The same vector unrolling method was previously used in the neural networks programming exercise.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    \n",
    "   Steps for collaborative filtering learning algorithm:\n",
    "   1. Initialize $x^{(1)}, \\dots, x^{(n_m)}$ and $\\theta^{(1)}, \\dots, \\theta^{(n_u)}$ to small random values\n",
    "   2. Minimize $J(x^{(1)}, \\dots, x^{(n_m)}$ and $\\theta^{(1)}, \\dots, \\theta^{(n_u)})$ using gradient descent or an advanced optimization algorithm for every $j = 1, \\dots, n_u , i = 1, \\dots, n_m: \\\\$ \n",
    "   $\\begin{align*} x_k^{(i)} := x_k^{(i)} - \\alpha \\sum_{j:r(i,j)=1} ( (\\theta^{(j)})^T x^{(i)} - y^{(i,j)})\\theta_k^{(j)} + \\lambda x_k^{(i)}   \\\\   \\theta_k^{(j)} := \\theta_k^{(j)} - \\alpha \\sum_{i:r(i,j)=1} ( (\\theta^{(j)})^T x^{(i)} - y^{(i,j)})x_k^{(i)} + \\lambda \\theta_k^{(j)} \\end{align*}$\n",
    "   \n",
    "   3. For a user with parameter $\\theta$ and a movie with (learned) features $x$, predict a star rating of $\\theta^T x.$   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Collaborative ﬁltering cost function\n",
    "The collaborative filtering cost function (without regularization) is given by $\\begin{align*} J(x^{(1)}, \\dots, x^{(n_m)},\\theta^{(1)}, \\dots, \\theta^{(n_u)}) = \\frac{1}{2} \\sum_{(i,j):r(i,j)=1} ( (\\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 .\\end{align*}$\n",
    "\n",
    "For the code, return this cost in the variable $J$. Note the cost should be accumulated for user $j$ and movie $i$ only if $R(i,j)=1$. For the cost function, the expected output is 22.22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at loaded parameters: 22.224604 \n",
      "(this value should be about 22.22)\n"
     ]
    }
   ],
   "source": [
    "#define function for exercise(s)\n",
    "def cofiCostFunc(params, Y, R, num_users, num_movies, num_features, Lambda):\n",
    "#COFICOSTFUNC Collaborative filtering cost function\n",
    "#   J = COFICOSTFUNC(params, Y, R, num_users, num_movies, ...\n",
    "#   num_features, lambda) returns the cost for the\n",
    "#   collaborative filtering problem.\n",
    "    \n",
    "    #Unfold the U and W matrices from params\n",
    "    X = np.reshape(params[0:num_movies*num_features], (num_movies, num_features), order='F')\n",
    "    Theta = np.reshape(params[num_movies*num_features:], (num_users, num_features), order='F')\n",
    "\n",
    "    J = np.sum(np.square(np.multiply(np.dot(X,Theta.T)-Y,R)))/2\n",
    "    reg = (Lambda/2)*(np.sum(np.square(X)) + np.sum(np.square(Theta)))\n",
    "    \n",
    "    return J + reg\n",
    "    \n",
    "# ============ Part 2: Collaborative Filtering Cost Function ===========\n",
    "#  You will now implement the cost function for collaborative filtering.\n",
    "#  To help you debug your cost function, we have included set of weights\n",
    "#  that we trained on that. Specifically, you should complete the code in \n",
    "#  cofiCostFunc.m to return J.\n",
    "\n",
    "#Load pre-trained weights (X, Theta, num_users, num_movies, num_features)\n",
    "movie_params = sio.loadmat('ex8_movieParams.mat')\n",
    "X = movie_params['X']\n",
    "Theta = movie_params['Theta']\n",
    "\n",
    "#Reduce the data set size so that this runs faster\n",
    "num_users = 4 \n",
    "num_movies = 5 \n",
    "num_features = 3\n",
    "\n",
    "X = X[0:num_movies, 0:num_features]\n",
    "Theta = Theta[0:num_users, 0:num_features]\n",
    "Y = Y[0:num_movies, 0:num_users]\n",
    "R = R[0:num_movies, 0:num_users]\n",
    "\n",
    "#Evaluate cost function\n",
    "params = np.vstack((X.T.reshape((-1,1)),Theta.T.reshape((-1,1)))) \n",
    "J = cofiCostFunc(params, Y, R, num_users, num_movies, num_features, 0)\n",
    "\n",
    "print('Cost at loaded parameters: %f ' % J)\n",
    "print('(this value should be about 22.22)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Collaborative ﬁltering gradient\n",
    "Implement the gradient (without regularization). Complete the code in cofiCostFunc to return the variables X_grad and Theta_grad. Note that X_grad should be a matrix of the same size as $X$ and similarly, Theta_grad is a matrix of the same size as $Theta$. The gradients of the cost function is given by: $\\begin{align*} \\frac{\\partial J}{\\partial x_k^{(i)}} = \\sum_{j:r(i,j)=1} ( (\\theta^{(j)})^T x^{(i)} - y^{(i,j)})\\theta_k^{(j)}  \\\\  \\frac{\\partial J}{\\partial \\theta_k^{(j)} } = \\sum_{i:r(i,j)=1} ( (\\theta^{(j)})^T x^{(i)} - y^{(i,j)})x_k^{(i)}. \\end{align*}$\n",
    "\n",
    "Note that the function returns the gradient for both sets of variables by unrolling them into a single vector.  Run a gradient check (checkCostFunction) to numerically check the implementation of the gradients. This is similar to the numerical check used in the neural networks exercise.. If the implementation is correct, then the analytical and numerical gradients match up closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Gradients (without regularization) ... \n",
      "[[-5.98391181e+00 -5.98391181e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 4.41710118e-01  4.41710118e-01]\n",
      " [-2.79263368e+00 -2.79263368e+00]\n",
      " [ 8.65238975e+00  8.65238975e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [-1.04240082e+00 -1.04240082e+00]\n",
      " [-1.61347590e+00 -1.61347590e+00]\n",
      " [-1.82913851e+00 -1.82913851e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 5.78903463e-01  5.78903463e-01]\n",
      " [-1.00254156e+00 -1.00254156e+00]\n",
      " [ 1.99934051e+00  1.99934051e+00]\n",
      " [ 8.11395966e+00  8.11395966e+00]\n",
      " [ 4.25774720e+00  4.25774720e+00]\n",
      " [ 1.18516448e+01  1.18516448e+01]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 1.58798064e+00  1.58798064e+00]\n",
      " [ 2.29968758e-02  2.29968758e-02]\n",
      " [ 8.38218093e-01  8.38218093e-01]\n",
      " [ 1.63463099e+00  1.63463099e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [-2.21972878e+00 -2.21972878e+00]\n",
      " [-3.31117940e-01 -3.31117940e-01]\n",
      " [-9.52577173e-04 -9.52577177e-04]\n",
      " [ 1.29540799e+00  1.29540799e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "The above two columns you get should be very similar.\n",
      "[Left-Your Numerical Gradient   Right-Analytical Gradient]\n",
      "\n",
      "If your cost function implementation is correct, then\n",
      "the relative difference will be small (less than 1e-9).\n",
      "Relative Difference: 0.9415101422092721\n"
     ]
    }
   ],
   "source": [
    "#define function for exercise(s)\n",
    "def computeNumericalGradient(params, Y, R, num_users, num_movies, num_features, Lambda):\n",
    "#COMPUTENUMERICALGRADIENT Computes the gradient using \"finite differences\"\n",
    "#and gives us a numerical estimate of the gradient.\n",
    "#   numgrad = COMPUTENUMERICALGRADIENT(J, theta) computes the numerical\n",
    "#   gradient of the function J around theta. Calling y = J(theta) should\n",
    "#   return the function value at theta.\n",
    "\n",
    "# Notes: The following code implements numerical gradient checking, and \n",
    "#        returns the numerical gradient.It sets numgrad(i) to (a numerical \n",
    "#        approximation of) the partial derivative of J with respect to the \n",
    "#        i-th input argument, evaluated at theta. (i.e., numgrad(i) should \n",
    "#        be the (approximately) the partial derivative of J with respect \n",
    "#        to theta(i).)\n",
    "    numgrad = np.zeros((params.shape[0],params.shape[1]))\n",
    "    perturb = np.zeros((params.shape[0],params.shape[1]))\n",
    "    e = 1e-4\n",
    "    for p in range(0,params.size):\n",
    "        #Set perturbation vector\n",
    "        #change one of elements of the vector to be the value of e\n",
    "        perturb[p] = e\n",
    "        #calculate the cost function at the perturb values for theta\n",
    "        loss1 = cofiCostFunc(params - perturb, Y, R, num_users, num_movies, \n",
    "                             num_features, Lambda)\n",
    "        loss2 = cofiCostFunc(params + perturb, Y, R, num_users, num_movies, \n",
    "                             num_features, Lambda)\n",
    "        #Compute Numerical Gradient\n",
    "        numgrad[p] = (loss2 - loss1) / float(2*e)\n",
    "        #change the element back to zero\n",
    "        perturb[p] = 0\n",
    "        \n",
    "    return numgrad\n",
    "\n",
    "def cofiGradient(params, Y, R, num_users, num_movies, num_features, Lambda):\n",
    "#COFIGRADIENT Collaborative filtering gradient function\n",
    "#   grad = COFIGRADIENT(params, Y, R, num_users, num_movies, ...\n",
    "#   num_features, lambda) returns the gradient for the\n",
    "#   collaborative filtering problem.    \n",
    "    \n",
    "    # Unfold the U and W matrices from params\n",
    "    X = np.reshape(params[0:num_movies*num_features], (num_movies, num_features), order='F')\n",
    "    Theta = np.reshape(params[num_movies*num_features:], (num_users, num_features), order='F')\n",
    "    \n",
    "    X_grad = np.dot(np.multiply(np.dot(X,Theta.T)-Y,R), Theta) + (Lambda * X)\n",
    "    Theta_grad = np.dot((np.multiply(np.dot(X,Theta.T)-Y,R)).T, X) + (Lambda * Theta)\n",
    "    \n",
    "    grad = np.vstack((X_grad.T.reshape((-1,1)),Theta_grad.T.reshape((-1,1))))\n",
    "    #use for method = TNC:\n",
    "    #return grad\n",
    "    \n",
    "    #change G.shape from 2d to 1d for bgfs, cg, etc:\n",
    "    return np.ravel(grad)\n",
    "    \n",
    "    \n",
    "def checkCostFunction(Lambda):\n",
    "#CHECKCOSTFUNCTION Creates a collaborative filering problem \n",
    "#to check your cost function and gradients\n",
    "#   CHECKCOSTFUNCTION(lambda) Creates a collaborative filering problem \n",
    "#   to check your cost function and gradients, it will output the \n",
    "#   analytical gradients produced by your code and the numerical gradients \n",
    "#   (computed using computeNumericalGradient). These two gradient \n",
    "#   computations should result in very similar values.\n",
    "    \n",
    "    #Create small problem\n",
    "    X_t = np.random.rand(4, 3)\n",
    "    Theta_t = np.random.rand(5,3)\n",
    "    \n",
    "    #Zap out most entries\n",
    "    Y = np.dot(X_t, Theta_t.T)\n",
    "    Y[np.random.rand(Y.shape[0], Y.shape[1]) > 0.5] = 0\n",
    "    R = (Y>0)*1\n",
    "    \n",
    "    #Run Gradient Checking\n",
    "    X = np.random.randn(X_t.shape[0], X_t.shape[1])\n",
    "    Theta = np.random.randn(Theta_t.shape[0], Theta_t.shape[1])\n",
    "    num_users = Y.shape[1]\n",
    "    num_movies = Y.shape[0]\n",
    "    num_features = Theta_t.shape[1]\n",
    "    params = np.vstack((X.T.reshape((-1,1)),Theta.T.reshape((-1,1)))) \n",
    "    \n",
    "    numgrad = computeNumericalGradient(params, Y, R, num_users, num_movies, \n",
    "                                       num_features, Lambda)\n",
    "    \n",
    "    grad = cofiGradient(params, Y, R, num_users, num_movies, num_features, Lambda)\n",
    "    \n",
    "    #Visually examine the two gradient computations.  The two columns\n",
    "    #you get should be very similar. \n",
    "    print(np.column_stack((numgrad, grad)))\n",
    "    print('\\nThe above two columns you get should be very similar.') \n",
    "    print('[Left-Your Numerical Gradient   Right-Analytical Gradient]')\n",
    "\n",
    "    #Evaluate the norm of the difference between two solutions.  \n",
    "    #If you have a correct implementation, and assuming you used EPSILON = 0.0001 \n",
    "    #in computeNumericalGradient.m, then diff below should be less than 1e-9\n",
    "    diff = np.linalg.norm(numgrad-grad)/(np.linalg.norm(numgrad+grad))\n",
    "    #numpy.linalg.norm is the euclidean norm or 2-norm\n",
    "    \n",
    "    print('\\nIf your cost function implementation is correct, then') \n",
    "    print('the relative difference will be small (less than 1e-9).') \n",
    "    print('Relative Difference:', diff)\n",
    "\n",
    "# ============== Part 3: Collaborative Filtering Gradient ==============\n",
    "#  Once your cost function matches up with ours, you should now implement \n",
    "#  the collaborative filtering gradient function. Specifically, you should \n",
    "#  complete the code in cofiCostFunc.m to return the grad argument.\n",
    "  \n",
    "print('Checking Gradients (without regularization) ... ')\n",
    "\n",
    "#Check gradients by running checkNNGradients\n",
    "checkCostFunction(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Regularized cost function\n",
    "The cost function for collaborative filtering with regularization is given by\n",
    "$\\begin{align*} J(x^{(1)}, \\dots, x^{(n_m)},\\theta^{(1)}, \\dots, \\theta^{(n_u)}) = \\frac{1}{2} \\sum_{(i,j):r(i,j)=1} ( (\\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \\left (\\frac{\\lambda}{2} \\sum_{j=1}^{n_u}\\sum_{k=1}^n (\\theta_k^{(j)})^2 \\right)  + \\left (\\frac{\\lambda}{2} \\sum_{i=1}^{n_m}\\sum_{k=1}^n (x_k^{(i)})^2 \\right). \\end{align*}$\n",
    "\n",
    "The regularization terms are added to the original computations of the cost function, $J$. Run the regularized cost function to see an expected cost of about $31.34.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at loaded parameters (lambda = 1.5): 31.344056\n",
      "(this value should be about 31.34)\n"
     ]
    }
   ],
   "source": [
    "# ========= Part 4: Collaborative Filtering Cost Regularization ========\n",
    "#  Now, you should implement regularization for the cost function for \n",
    "#  collaborative filtering. You can implement it by adding the cost of\n",
    "#  regularization to the original cost computation.\n",
    "\n",
    "#Evaluate cost function\n",
    "J = cofiCostFunc(params, Y, R, num_users, num_movies, num_features, 1.5)\n",
    "\n",
    "print('Cost at loaded parameters (lambda = 1.5): %f' % J)\n",
    "print('(this value should be about 31.34)') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Regularized gradient\n",
    "Proceed to implement regularization for the gradient. Return the regularized gradient by adding the contributions from the regularization terms. Note that the gradients for the regularized cost function is given by:\n",
    "$\\begin{align*} \\frac{\\partial J}{\\partial x_k^{(i)}} = \\sum_{j:r(i,j)=1} ( (\\theta^{(j)})^T x^{(i)} - y^{(i,j)})\\theta_k^{(j)} + \\lambda x_k^{(i)}   \\\\  \\frac{\\partial J}{\\partial \\theta_k^{(j)} } = \\sum_{i:r(i,j)=1} ( (\\theta^{(j)})^T x^{(i)} - y^{(i,j)})x_k^{(i)} + \\lambda \\theta_k^{(j)}. \\end{align*}$\n",
    "\n",
    "This means that $\\lambda x^{(i)}$ is added to X_grad(i,:) and $\\lambda \\theta^{(j)}$ is added to Theta_grad(j,:).\n",
    "\n",
    "Run another gradient check (checkCostFunction) to numerically check the implementation of the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Gradients (with regularization) ... \n",
      "[[-1.98836014e+00 -1.98836014e+00]\n",
      " [-2.84799187e+00 -2.84799187e+00]\n",
      " [-3.71882360e+00 -3.71882360e+00]\n",
      " [ 9.49997233e+00  9.49997233e+00]\n",
      " [ 1.82294729e+00  1.82294729e+00]\n",
      " [-1.35933953e+00 -1.35933953e+00]\n",
      " [-2.97657768e-01 -2.97657768e-01]\n",
      " [ 4.39997405e+00  4.39997405e+00]\n",
      " [-1.75461567e+00 -1.75461567e+00]\n",
      " [ 7.43641974e-01  7.43641974e-01]\n",
      " [-2.65101553e+00 -2.65101553e+00]\n",
      " [-7.65258787e+00 -7.65258787e+00]\n",
      " [ 4.01784535e+00  4.01784535e+00]\n",
      " [ 1.10609330e+01  1.10609330e+01]\n",
      " [-1.03966279e+00 -1.03966279e+00]\n",
      " [-1.12229698e+00 -1.12229698e+00]\n",
      " [ 8.32512664e+00  8.32512664e+00]\n",
      " [ 8.63435812e-04  8.63435820e-04]\n",
      " [ 7.45677294e-01  7.45677294e-01]\n",
      " [ 5.47558064e-01  5.47558064e-01]\n",
      " [ 5.94531749e-01  5.94531749e-01]\n",
      " [-9.64480187e-01 -9.64480187e-01]\n",
      " [-2.34407745e+00 -2.34407745e+00]\n",
      " [-8.47488288e+00 -8.47488288e+00]\n",
      " [ 2.99614815e-01  2.99614815e-01]\n",
      " [ 2.12271587e+00  2.12271587e+00]\n",
      " [-9.87297184e-01 -9.87297184e-01]]\n",
      "\n",
      "The above two columns you get should be very similar.\n",
      "[Left-Your Numerical Gradient   Right-Analytical Gradient]\n",
      "\n",
      "If your cost function implementation is correct, then\n",
      "the relative difference will be small (less than 1e-9).\n",
      "Relative Difference: 0.9964359425619481\n"
     ]
    }
   ],
   "source": [
    "# ======= Part 5: Collaborative Filtering Gradient Regularization ======\n",
    "#  Once your cost matches up with ours, you should proceed to implement \n",
    "#  regularization for the gradient.\n",
    "\n",
    "print('Checking Gradients (with regularization) ... ')\n",
    "\n",
    "#Check gradients by running checkNNGradients\n",
    "checkCostFunction(1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Learning movie recommendations\n",
    "\n",
    "After finishing the implementation the collaborative filtering cost function and gradient, start training the algorithm to make movie recommendations for yourself. Enter your own movie preferences, so that later when the algorithm runs, you can get your own movie recommendations! Some values have been filled out according to the assignment preferences, but this should be changed according to your own tastes. The list of all movies and their number in the dataset can be found listed in the file movie_idx.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New user ratings:\n",
      "\n",
      "Rated 4 for Toy Story (1995)\n",
      "\n",
      "Rated 3 for Twelve Monkeys (1995)\n",
      "\n",
      "Rated 5 for Usual Suspects, The (1995)\n",
      "\n",
      "Rated 4 for Outbreak (1995)\n",
      "\n",
      "Rated 5 for Shawshank Redemption, The (1994)\n",
      "\n",
      "Rated 3 for While You Were Sleeping (1995)\n",
      "\n",
      "Rated 5 for Forrest Gump (1994)\n",
      "\n",
      "Rated 2 for Silence of the Lambs, The (1991)\n",
      "\n",
      "Rated 4 for Alien (1979)\n",
      "\n",
      "Rated 5 for Die Hard 2 (1990)\n",
      "\n",
      "Rated 5 for Sphere (1998)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define function for exercise(s)\n",
    "def loadMovieList():\n",
    "#GETMOVIELIST reads the fixed movie list in movie.txt and returns a\n",
    "#cell array of the words\n",
    "#   movieList = GETMOVIELIST() reads the fixed movie list in movie.txt \n",
    "#   and returns a cell array of the words in movieList.\n",
    "    \n",
    "    #Read the fixed movieulary list\n",
    "    fid = open('movie_ids.txt')\n",
    "\n",
    "    #Store all movies in cell array movie{} \n",
    "    movieList = {}\n",
    "    #loop through each line\n",
    "    for line in fid:\n",
    "        string = line.split(' ')\n",
    "        idx = int(string[0])\n",
    "        movieName = ' '.join(string[1:])\n",
    "        #strip white spaces\n",
    "        movieList[idx-1] = movieName.strip()\n",
    "        \n",
    "    fid.close()\n",
    "    return movieList\n",
    "\n",
    "# ============== Part 6: Entering ratings for a new user ===============\n",
    "#  Before we will train the collaborative filtering model, we will first\n",
    "#  add ratings that correspond to a new user that we just observed. This\n",
    "#  part of the code will also allow you to put in your own ratings for the\n",
    "#  movies in our dataset!\n",
    "\n",
    "movieList = loadMovieList()\n",
    "\n",
    "#Initialize my ratings\n",
    "my_ratings = np.zeros((1682, 1))\n",
    "\n",
    "#Check the file movie_idx.txt for id of each movie in our dataset\n",
    "#For example, Toy Story (1995) has ID 1, so to rate it \"4\", you can set\n",
    "my_ratings[0] = 4\n",
    "#Or suppose did not enjoy Silence of the Lambs (1991), you can set\n",
    "my_ratings[97] = 2\n",
    "\n",
    "#We have selected a few movies we liked / did not like and the ratings we\n",
    "#gave are as follows:\n",
    "my_ratings[6] = 3  \n",
    "my_ratings[11] = 5  \n",
    "my_ratings[53] = 4  \n",
    "my_ratings[63] = 5  \n",
    "my_ratings[65] = 3  \n",
    "my_ratings[68] = 5    \n",
    "my_ratings[182] = 4  \n",
    "my_ratings[225] = 5  \n",
    "my_ratings[354] = 5\n",
    "\n",
    "print('New user ratings:\\n')\n",
    "for i in range(len(my_ratings)):\n",
    "    if my_ratings[i] > 0:\n",
    "        print('Rated %d for %s\\n' % (my_ratings[i], movieList[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender system learning completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define function for exercise(s)\n",
    "def normalizeRatings(Y, R):\n",
    "#NORMALIZERATINGS Preprocess data by subtracting mean rating for every \n",
    "#movie (every row)\n",
    "#   [Ynorm, Ymean] = NORMALIZERATINGS(Y, R) normalized Y so that each movie\n",
    "#   has a rating of 0 on average, and returns the mean rating in Ymean.\n",
    "    m, n = Y.shape\n",
    "    Ymean = np.zeros((m, 1))  \n",
    "    Ynorm = np.zeros((m, n))\n",
    "    for i in range(m):\n",
    "        idx = np.where(R[i,:] == 1)[0]\n",
    "        Ymean[i] = Y[i,idx].mean()\n",
    "        Ynorm[i,idx] = Y[i,idx] - Ymean[i]\n",
    "    \n",
    "    return Ynorm, Ymean\n",
    "\n",
    "# ================== Part 7: Learning Movie Ratings ====================\n",
    "#  Now, you will train the collaborative filtering model on a movie rating \n",
    "#  dataset of 1682 movies and 943 users\n",
    "\n",
    "#Load data\n",
    "movies = sio.loadmat('ex8_movies.mat')\n",
    "R = movies['R']\n",
    "Y = movies['Y']\n",
    "\n",
    "#Y is a 1682x943 matrix, containing ratings (1-5) of 1682 movies by \n",
    "#943 users\n",
    "\n",
    "#R is a 1682x943 matrix, where R(i,j) = 1 if and only if user j gave a\n",
    "#rating to movie i\n",
    "\n",
    "#Add our own ratings to the data matrix\n",
    "Y = np.column_stack((my_ratings, Y))\n",
    "R = np.column_stack((my_ratings>0, R))\n",
    "\n",
    "#Normalize Ratings\n",
    "Ynorm, Ymean = normalizeRatings(Y, R)\n",
    "\n",
    "#Useful Values\n",
    "num_users = Y.shape[1]\n",
    "num_movies = Y.shape[0]\n",
    "num_features = 10\n",
    "\n",
    "#Set Initial Parameters (Theta, X)\n",
    "X = np.random.randn(num_movies, num_features)\n",
    "Theta = np.random.randn(num_users, num_features)\n",
    "\n",
    "initial_parameters = np.vstack((X.T.reshape((-1,1)),Theta.T.reshape((-1,1)))) \n",
    "\n",
    "#Set Regularization\n",
    "Lambda=10\n",
    "fmin = sp.optimize.minimize(fun=cofiCostFunc, x0=initial_parameters, \n",
    "                            args=(Ynorm,R,num_users,num_movies,num_features,Lambda), \n",
    "                            method='CG', jac=cofiGradient, \n",
    "                            options={'maxiter': 100})\n",
    "\n",
    "#Unfold the returned theta back into U and W\n",
    "X = np.reshape(fmin.x[0:num_movies*num_features], (num_movies, num_features), order='F')\n",
    "Theta = np.reshape(fmin.x[num_movies*num_features:], (num_users, num_features), order='F')\n",
    "    \n",
    "print('Recommender system learning completed.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    Mean normalization is a preprocessing step for collaborative filtering, in which each row of the matrix $Y$ is normalized to have mean zero. When having a new user or a user that hasn't rated anything, this helps to avoid predicting that that user will rate everything with zero stars. \n",
    "    \n",
    "  In the case where some movies have no rating, then it might be better to normalize the different columns to have mean zero instead of normalizing the rows to have mean zero. Yet, maybe a movie with no ratings should not be recommended regardless. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Recommendations\n",
    "After the additional ratings have been added to the dataset, the code will proceed to train the collaborative filtering model to learn the parameters $X$ and $Theta.$ To predict the rating of movie $i$ for user $j$, compute $(\\theta^{(j)})^T x^{(i)}.$ The next part of the code computes the ratings for all the movies and users and displays the movies that it recommends (Figure 4), according to ratings that were entered earlier in the script. Note that a different set of the predictions might be obtained due to different random initializations.\n",
    "\n",
    "Image Source: ex8.pdf from Andrew Ng's Coursera course in Machine Learning\n",
    "![mov_rec.png](mov_rec.png)\n",
    "\n",
    "The \"Top recommendations for you\" in the image uses 1-10 ratings instead of 1-5 ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommendations for you:\n",
      "\n",
      "Predicting rating 5.0 for movie Star Kid (1997)\n",
      "\n",
      "Predicting rating 5.0 for movie Prefontaine (1997)\n",
      "\n",
      "Predicting rating 5.0 for movie Great Day in Harlem, A (1994)\n",
      "\n",
      "Predicting rating 5.0 for movie Aiqing wansui (1994)\n",
      "\n",
      "Predicting rating 5.0 for movie Entertaining Angels: The Dorothy Day Story (1996)\n",
      "\n",
      "Predicting rating 5.0 for movie They Made Me a Criminal (1939)\n",
      "\n",
      "Predicting rating 5.0 for movie Saint of Fort Washington, The (1993)\n",
      "\n",
      "Predicting rating 5.0 for movie Someone Else's America (1995)\n",
      "\n",
      "Predicting rating 5.0 for movie Marlene Dietrich: Shadow and Light (1996)\n",
      "\n",
      "Predicting rating 5.0 for movie Santa with Muscles (1996)\n",
      "\n",
      "Original ratings provided:\n",
      "\n",
      "Rated 4 for Toy Story (1995)\n",
      "\n",
      "Rated 3 for Twelve Monkeys (1995)\n",
      "\n",
      "Rated 5 for Usual Suspects, The (1995)\n",
      "\n",
      "Rated 4 for Outbreak (1995)\n",
      "\n",
      "Rated 5 for Shawshank Redemption, The (1994)\n",
      "\n",
      "Rated 3 for While You Were Sleeping (1995)\n",
      "\n",
      "Rated 5 for Forrest Gump (1994)\n",
      "\n",
      "Rated 2 for Silence of the Lambs, The (1991)\n",
      "\n",
      "Rated 4 for Alien (1979)\n",
      "\n",
      "Rated 5 for Die Hard 2 (1990)\n",
      "\n",
      "Rated 5 for Sphere (1998)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================== Part 8: Recommendation for you ====================\n",
    "#  After training the model, you can now make recommendations by computing\n",
    "#  the predictions matrix.\n",
    "\n",
    "p = np.dot(X,Theta.T)\n",
    "my_predictions = p[:,0].reshape((-1,1)) + Ymean\n",
    "\n",
    "ix = np.argsort(my_predictions, axis=0)[::-1] #sort in desending order\n",
    "print('Top recommendations for you:\\n')\n",
    "for i in range(10):\n",
    "    j = int(ix[i])\n",
    "    print('Predicting rating %.1f for movie %s\\n' % (my_predictions[j], movieList[j]))\n",
    "\n",
    "print('Original ratings provided:\\n')\n",
    "for i in range(len(my_ratings)):\n",
    "    if my_ratings[i] > 0:\n",
    "        print('Rated %d for %s\\n' % (my_ratings[i], movieList[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
